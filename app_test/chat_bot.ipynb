{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the PDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from PDF file\n",
    "def data_extractor(dir_path):\n",
    "    loader = DirectoryLoader(dir_path, # directory path\n",
    "                            glob=\"*.pdf\", # only pdf files\n",
    "                            loader_cls = PyPDFLoader, # using module\n",
    "                            show_progress=True,\n",
    "                            use_multithreading=True\n",
    "                            )\n",
    "    # loading the pdf documents\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data = data_extractor(\"D:\\Projects\\Medical-Chat-Bot-using-Llama-2\\data\\\\\")\n",
    "# print(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting corpus into text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text chunks\n",
    "def text_splitter(document):\n",
    "    text_chunks = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                    chunk_overlap = 20\n",
    "                                )\n",
    "    return text_chunks.Create_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_splitter(pdf_data)\n",
    "print('Length of text chunks : ',len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks[524].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting text to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the embedding model from hugging face\n",
    "EMBD_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=EMBD_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "PINE_CONE_API_KEY = os.environ.get('PINE_CONE_API_KEY')\n",
    "PINE_CONE_ENV = os.environ.get('PINE_CONE_ENV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the vector DB pinecone\n",
    "pinecone.init(api_key=PINE_CONE_API_KEY,\n",
    "              environment=PINE_CONE_ENV\n",
    "            )\n",
    "index_name = \"medical-chat-bot-llama-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating embeddings for each text chunk and storing into pinecone\n",
    "doc_search = Pinecone.from_texts([i.page_content for i in text_chunks], # list of chunks text\n",
    "                                  embedding_model, # Embedding model \n",
    "                                  index_name = index_name  # pinecone index name\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the existing index from pinecone\n",
    "doc_search = Pinecone.from_existing_index(index_name,embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type='llama',\n",
    "model = CTransformers(model=\"TheBloke/Llama-2-7B-Chat-GGML\",\n",
    "                    model_file='llama-2-7b-chat.ggmlv3.q5_0.bin',\n",
    "                    device_map='cuda',\n",
    "                    config = {'temperature':0.35,\n",
    "                            'max_new_tokens':1024,\n",
    "                            # 'repetition_penalty':0.7,\n",
    "                            'stream':True,\n",
    "                            # 'context_length':-1\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the prompt\n",
    "prompt_template=\"\"\"\n",
    "                Use the following pieces of information to answer the user's question.\n",
    "                If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "                Context: {context}\n",
    "                Question: {question}\n",
    "\n",
    "                Only return the helpful answer below and nothing else.\n",
    "                Helpful answer:\n",
    "                \"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template,\n",
    "                        input_variables=['context','question'],\n",
    "                        )\n",
    "chain_type_kwargs = {'prompt':prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_bot = RetrievalQA.from_chain_type(llm=model,\n",
    "                                        chain_type=\"stuff\",\n",
    "                                        retriever = doc_search.as_retriever(search_kwargs={'k':4}),\n",
    "                                        return_source_documents=True,\n",
    "                                        chain_type_kwargs=chain_type_kwargs\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_bot({\"query\": 'Hello'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical_chat_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
